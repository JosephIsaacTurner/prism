import numpy as np
from scipy.stats import genpareto, kstest
from tqdm import tqdm
from statsmodels.stats.multitest import fdrcorrection
from .loading import apply_tfce, load_nifti_if_not_already_nifti, is_nifti_like, Dataset
from nilearn.maskers import NiftiMasker
import nibabel as nib
from jax import jit, random
import warnings
from typing import List, Optional, Union, Tuple


def permutation_analysis(data, design, contrast, stat_function, n_permutations, random_state, two_tailed=True, exchangeability_matrix=None, vg_auto=False, vg_vector=None, within=True, whole=False, flip_signs=False, accel_tail=True, on_permute_callback=None):
    """
    Performs permutation testing on the provided data using a specified statistical function.
    
    The function calculates the observed (true) test statistics, then generates a null distribution
    by permuting the design matrix (optionally respecting an exchangeability structure). It computes:
      - Empirical (uncorrected) p-values,
      - FDR-corrected p-values via the Benjamini-Hochberg procedure,
      - FWE-corrected p-values using a max-statistic approach (with an option for accelerated tail estimation via a GPD fit).
    
    Parameters
    ----------
    data : np.ndarray, shape (n_samples, n_elements_per_sample)
        The data matrix, where each row represents a sample and each column an element/feature.
    design : np.ndarray, shape (n_samples, n_features)
        The design matrix for the GLM, where each row corresponds to a sample and each column to a regressor.
    contrast : np.ndarray, shape (n_features,) or (n_contrasts, n_features)
        The contrast vector or matrix specifying the hypothesis to be tested.
    stat_function : function
        A function that calculates the test statistic. It must accept the arguments (data, design, contrast)
        and return an array of test statistics. The shape of its output defines the shape of the result.
    n_permutations : int
        The number of permutations to perform.
    random_state : int
        Seed for the random number generator to ensure reproducibility.
    two_tailed : bool, default True
        If True, p-values are computed in a two-tailed manner using absolute values of statistics.
    exchangeability_matrix : np.ndarray, optional
        Defines the exchangeability blocks for permutation testing. Expected shapes:
          - If provided as a vector: (n_samples,) or (n_samples, 1).
          - If provided as a matrix: (n_samples, n_permutation_groups).
    vg_auto : bool, default False
        If True, automatically generates a variance group (VG) vector based on the exchangeability matrix.
        If False, the user must provide a VG vector via `vg_vector` if they want to define variance groups.
    vg_vector : np.ndarray, optional
        A 1D array defining the variance groups for each observation. If exchangeability blocks provided and vg_auto is True, vg_vector is calculated automatically.
    within : bool, default True
        For a 1D exchangeability matrix, indicates whether to permute within blocks.
    whole : bool, default False
        For a 1D exchangeability matrix, indicates whether to permute whole blocks.
    flip_signs : bool, default False
        If True, randomly flips the signs of the data for each permutation (assume independent and symmetric errors (ISE)).
    accel_tail : bool, default True
        If True, applies the accelerated tail method (GPD approximation) to compute p-values for cases with 
        low empirical exceedance counts.
    on_permute_callback: function (optional)
       If provided, calls a function on the permuted stats for each permutations.
    
    Returns
    -------
    unc_p : np.ndarray
        Uncorrected p-values computed empirically from the permutation distribution. 
        Shape matches that of the test statistic returned by `stat_function`.
    fdr_p : np.ndarray
        P-values corrected for multiple comparisons using the Benjamini-Hochberg procedure.
        Shape matches that of the test statistic returned by `stat_function`.
    fwe_p : np.ndarray or float
        Family-wise error rate (FWE) corrected p-values computed using the max-statistic approach.
        If `two_tailed` is True, the p-values are based on absolute values of statistics.
    
    Notes
    -----
    - The first dimension of both `data` and `design` is assumed to correspond to the samples (subjects).
    - The true test statistic is computed once using `stat_function`, and the null distribution is built by
      applying the same function on permuted versions of the design matrix generated by `yield_permuted_stats`.
    """
    # Step One: Calculate the true statistics
    if (exchangeability_matrix is not None and vg_auto) or vg_vector:
        if vg_vector is None:
            vg_vector = get_vg_vector(exchangeability_matrix, within=within, whole=whole)
        true_stats = stat_function(data, design, contrast, vg_vector, len(np.unique(vg_vector)))
    else:
        true_stats = stat_function(data, design, contrast)
    # Step Two: Run the permutations
    exceedances = np.zeros(true_stats.shape)
    permutation_generator = yield_permuted_stats(
        data, design, contrast,
        stat_function=stat_function,
        n_permutations=n_permutations,
        random_state=random_state,
        exchangeability_matrix=exchangeability_matrix,
        vg_auto=vg_auto,
        vg_vector=vg_vector,
        within=within,
        whole=whole,
        flip_signs=flip_signs
    )
    for i in tqdm(range(n_permutations), desc="Permuting..."):
        permuted_stats = next(permutation_generator)
        if on_permute_callback is not None:
            on_permute_callback(permuted_stats, i, two_tailed)
        if two_tailed:
            exceedances += np.abs(permuted_stats) >= np.abs(true_stats)
            max_stat_dist = np.hstack([np.max(np.abs(permuted_stats)), max_stat_dist]) if i > 0 else np.max(np.abs(permuted_stats))
        else:
            exceedances += permuted_stats >= true_stats
            max_stat_dist = np.hstack([np.max(permuted_stats), max_stat_dist]) if i > 0 else np.max(permuted_stats)
    # Step Three: Calculate uncorrected p-values
    unc_p = (exceedances + 1) / (n_permutations + 1)
    # Step Four: Correct using FDR (Benjamini-Hochberg)
    fdr_p = fdrcorrection(unc_p)[1]
    # Step Five: Correct using FWE (max-stat i.e. Westfall-Young)
    if accel_tail:
        # Use a generalized Pareto distribution to estimate p-values for the tail.
        fwe_p = compute_p_values_accel_tail(true_stats, max_stat_dist, two_tailed=two_tailed)
    else:
        if two_tailed:
            fwe_p = (np.sum(max_stat_dist[None, :] >= np.abs(true_stats[:, None]), axis=1) + 1) / (n_permutations + 1)
        else:
            fwe_p = (np.sum(max_stat_dist[None, :] >= true_stats[:, None], axis=1) + 1) / (n_permutations + 1)
    return unc_p, fdr_p, fwe_p


def permutation_analysis_volumetric_dense(imgs, mask_img,
                                          design, contrast, 
                                          stat_function, n_permutations, random_state,
                                          two_tailed=True, exchangeability_matrix=None, vg_auto=False, vg_vector=None,
                                          within=True, whole=False, flip_signs=False,
                                          accel_tail=True,
                                          tfce=True,
                                          save_1minusp=True,
                                          save_neglog10p=False):
    """
    Parameters
    ----------
    imgs : list of str
        List of file paths to the volumetric images (NIfTI format). Can also be a single file path for a 4d image (one per subject).
    mask_img : str
        File path to the mask image (NIfTI format).
    design : np.ndarray, shape (n_samples, n_features)
        The design matrix for the GLM, where each row corresponds to a sample and each column to a regressor.
    contrast : np.ndarray, shape (n_features,) or (n_contrasts, n_features)
        The contrast vector or matrix specifying the hypothesis to be tested.
    stat_function : function
        A function that calculates the test statistic. It must accept the arguments (data, design, contrast)
        and return an array of test statistics. The shape of its output defines the shape of the result.
    n_permutations : int
        The number of permutations to perform.
    random_state : int
        Seed for the random number generator to ensure reproducibility.
    two_tailed : bool, default True
        If True, p-values are computed in a two-tailed manner using absolute values of statistics.
    exchangeability_matrix : np.ndarray, optional
        Defines the exchangeability blocks for permutation testing. Expected shapes:
          - If provided as a vector: (n_samples,) or (n_samples, 1).
          - If provided as a matrix: (n_samples, n_permutation_groups).
    vg_auto : bool, default False
        If True, automatically generates a variance group (VG) vector based on the exchangeability matrix.
        If False, the user must provide a VG vector via `vg_vector` if they want to define variance groups.
    vg_vector : np.ndarray, optional
        A 1D array defining the variance groups for each observation. If exchangeability blocks provided and vg_auto is True, vg_vector is calculated automatically.
    within : bool, default True
        For a 1D exchangeability matrix, indicates whether to permute within blocks.
    whole : bool, default False
        For a 1D exchangeability matrix, indicates whether to permute whole blocks.
    flip_signs : bool, default False
        If True, randomly flips the signs of the data for each permutation (assume independent and symmetric errors (ISE)).
    accel_tail : bool, default True
        If True, applies the accelerated tail method (GPD approximation) to compute p-values for cases with 
        low empirical exceedance counts.
    tfce : bool, default True
        If True, applies Threshold-Free Cluster Enhancement (TFCE) to the test statistics.
    """
    # Step One: Load volumetric images into a 2d matrix (n_samples x n_voxels)
    if mask_img is None:
        print("Warning: No mask image provided. Using the whole image. Unexpected results may occur.")
        masker = NiftiMasker()
    else:
        masker = NiftiMasker(mask_img=mask_img)
    data = masker.fit_transform(imgs)
    results = {}

    # Step Two: Compute the true test statistics
    if (exchangeability_matrix is not None and vg_auto) or vg_vector:
        if vg_vector is None:
            vg_vector = get_vg_vector(exchangeability_matrix, within=within, whole=whole)
        true_stats = stat_function(data, design, contrast, vg_vector, len(np.unique(vg_vector)))
    else:
        true_stats = stat_function(data, design, contrast)
    
    # Initialize TFCE manager if needed
    if tfce:
        tfce_manager = TfceStatsManager(true_stats, load_nifti_if_not_already_nifti(mask_img), two_tailed=two_tailed)
    
    # Save the true map
    results["true_map"] = masker.inverse_transform(true_stats)
    
    # Step Three: Run permutation analysis
    # Here, the on_permute_callback is replaced with a direct update to tfce_manager
    def on_permute_callback(permuted_stats, permutation_idx, *args, **kwargs):
        if tfce:
            tfce_manager.update(permuted_stats, permutation_idx)
    
    unc_p, fdr_p, fwe_p = permutation_analysis(
        data, design, contrast, stat_function, n_permutations, random_state,
        two_tailed, exchangeability_matrix, vg_auto, vg_vector,
        within, whole, flip_signs, accel_tail,
        on_permute_callback
    )

    if tfce:
        unc_p_tfce, fdr_p_tfce, fwe_p_tfce = tfce_manager.finalize(n_permutations, accel_tail=accel_tail)
    
    if save_1minusp:
        unc_p = 1 - unc_p
        fdr_p = 1 - fdr_p
        fwe_p = 1 - fwe_p
        if tfce:
            unc_p_tfce = 1 - unc_p_tfce
            fdr_p_tfce = 1 - fdr_p_tfce
            fwe_p_tfce = 1 - fwe_p_tfce

    elif save_neglog10p:
        unc_p = -np.log10(unc_p)
        fdr_p = -np.log10(fdr_p)
        fwe_p = -np.log10(fwe_p)
        if tfce:
            unc_p_tfce = -np.log10(unc_p_tfce)
            fdr_p_tfce = -np.log10(fdr_p_tfce)
            fwe_p_tfce = -np.log10(fwe_p_tfce)

    results["unc_p_map"] = masker.inverse_transform(unc_p)
    results["fdr_p_map"] = masker.inverse_transform(fdr_p)
    results["fwe_p_map"] = masker.inverse_transform(fwe_p)
    
    # Step Four: If tfce is desired, finalize and save the tfce maps
    if tfce:
        results["true_tfce_map"] = masker.inverse_transform(tfce_manager.true_stats_tfce)
        results["unc_p_tfce_map"] = masker.inverse_transform(unc_p_tfce)
        results["fdr_p_tfce_map"] = masker.inverse_transform(fdr_p_tfce)
        results["fwe_p_tfce_map"] = masker.inverse_transform(fwe_p_tfce)
    
    return results


def spatial_correlation_permutation_analysis(
    datasets: Union[Dataset, List[Dataset]],
    reference_maps: Optional[Union[str, nib.Nifti1Image, np.ndarray, List[Union[str, nib.Nifti1Image, np.ndarray]]]] = None,
    two_tailed: bool = True
    ) -> Optional[dict]:
    """
    Computes spatial correlations between dataset statistic maps and reference maps,
    using permutation testing to assess significance.

    Parameters
    ----------
    datasets : Dataset or list of Dataset objects
        The datasets to compare. Each Dataset object should contain necessary
        parameters for statistic calculation and permutations.
    reference_maps : NIfTI path/object, np.ndarray, or list thereof, optional
        Reference maps to compare against. Must be compatible (in feature space)
        with the dataset statistic maps after potential masking.
    two_tailed : bool, default True
        If True, computes two-tailed p-values. If False, computes one-tailed
        (right-tailed) p-values.

    Returns
    -------
    results : dict or None
        A dictionary containing the results, or None if the analysis cannot proceed
        (e.g., due to insufficient inputs). The dictionary contains:
        - 'corr_matrix_ds_ds': (N_datasets x N_datasets) array of true correlations, or None.
        - 'corr_matrix_ds_ref': (N_datasets x N_references) array of true correlations, or None.
        - 'p_matrix_ds_ds': (N_datasets x N_datasets) array of p-values (diag=NaN), or None.
        - 'p_matrix_ds_ref': (N_datasets x N_references) array of p-values, or None.
        - 'corr_matrix_perm_ds_ds': (N_perm x N_datasets x N_datasets) array, or None.
        - 'corr_matrix_perm_ds_ref': (N_perm x N_datasets x N_references) array, or None.
    """
    analyzer = _SpatialCorrelationAnalysis(datasets, reference_maps, two_tailed)
    results = analyzer.run_analysis()
    return results


@jit
def flip_data(data, key):
    n_samples = data.shape[0]
    flip_signs = random.randint(key, (n_samples,), 0, 2) * 2 - 1  # Maps 0,1 to -1, +1.
    return data * flip_signs[:, None]


def yield_sign_flipped_data(data, n_permutations, random_state):
    """
    Generator function that yields sign-flipped versions of the input data one by one.

    Parameters:
      data : array-like, shape (n_samples, n_elements_per_sample)
          Input data matrix.
      n_permutations : int
          Number of sign-flipped permutations to generate.
      random_state : int
          Random seed for reproducibility.

    Yields:
      A sign-flipped version of data for each permutation.
    """
    key = random.PRNGKey(random_state)
    for _ in range(n_permutations):
        key, subkey = random.split(key)
        yield flip_data(data, subkey)


def yield_permuted_stats(data, design, contrast, stat_function, n_permutations, random_state, exchangeability_matrix=None, vg_auto=False, vg_vector=None, within=True, whole=False, flip_signs=False):
    """Generator function for permutation testing.
    data: Shape (n_samples, n_elements_per_sample)
    design: Shape (n_samples, n_features)
    contrast: Shape (n_features,) or (n_contrasts, n_features)
    stat_function: Function that calculates the test statistic. Must take data, design, contrast as arguments.
    n_permutations: Number of permutations to perform.
    random_state: Random seed for reproducibility.
    exchangeability_matrix (Optional): Exchangeability matrix for permutation testing. Shape (n_samples,) or (n_samples, n_permutation_groups).
    vg_auto (Optional): If True, automatically generates a variance group (VG) vector based on the exchangeability matrix.
    vg_vector (Optional): A 1D array defining the variance groups for each observation. If exchangeability blocks provided and vg_auto is True, vg_vector is calculated automatically.
    within (Optional): For a 1D exchangeability matrix, indicates whether to permute within blocks.
    whole (Optional): For a 1D exchangeability matrix, indicates whether to permute whole blocks.
    flip_signs (Optional): If True, randomly flips the signs of the data for each permutation.
    """
    calculate = stat_function
    permuted_design_generator = yield_permuted_design(design, n_permutations, random_state, exchangeability_matrix, within, whole)
    if flip_signs:
        sign_flipped_data_generator = yield_sign_flipped_data(data, n_permutations, random_state)
    for i in range(n_permutations):
        if flip_signs:
            data = next(sign_flipped_data_generator)
        if (exchangeability_matrix is not None and vg_auto) or vg_vector:
            if vg_vector is None:
                vg_vector = get_vg_vector(exchangeability_matrix, within=within, whole=whole)
            permuted_value = calculate(data, next(permuted_design_generator), contrast, vg_vector, len(np.unique(vg_vector)))
        else:
            permuted_value = calculate(data, next(permuted_design_generator), contrast)
        yield permuted_value


def permute_indices_recursive(current_original_indices, level, eb_matrix, rng, parent_instructed_fix_order=False):
    """
    Recursively permute indices based on the exchangeability matrix.
    Args:
        current_original_indices (np.ndarray): Current indices to permute.
        level (int): Current level in the exchangeability matrix.
        eb_matrix (np.ndarray): Exchangeability matrix.
        rng (np.random.Generator): Random number generator.
        parent_instructed_fix_order (bool): Whether the parent instructed to fix order.
    Returns:
        np.ndarray: Permuted indices.
    """
    if len(current_original_indices) == 0: return np.array([], dtype=int)
    if level >= eb_matrix.shape[1]: return rng.permutation(current_original_indices)

    is_last_defined_level = (level == eb_matrix.shape[1] - 1)
    current_eb_level_values = eb_matrix[current_original_indices, level]
    unique_blocks, block_inverse_indices = np.unique(current_eb_level_values, return_inverse=True)
    n_unique_blocks_at_level = len(unique_blocks)

    # --- Single Block Logic ---
    if n_unique_blocks_at_level == 1:
        block_val = unique_blocks[0]
        if block_val == 0:
             raise ValueError(f"Block index 0 found at level {level} for indices subset {current_original_indices[:5]}..., which is not supported.")

        if is_last_defined_level: # Explicit termination for last level
             if block_val > 0: return rng.permutation(current_original_indices)
             else: return np.copy(current_original_indices) # Neg@Last = Identity
        else: # Not last defined level - recurse
             instruct_fix_next = (block_val < 0) # Instruction for NEXT level
             if block_val > 0:
                  # Positive: Shuffle order of sub-blocks found at next level.
                  next_eb_level_values = eb_matrix[current_original_indices, level + 1]
                  unique_sub_blocks, sub_block_inverse, sub_block_counts = np.unique(next_eb_level_values, return_inverse=True, return_counts=True)
                  n_sub_blocks = len(unique_sub_blocks)
                  if n_sub_blocks <= 1:
                      # Pass parent_fix based on current block sign (False here)
                      return permute_indices_recursive(current_original_indices, level + 1, eb_matrix, rng, parent_instructed_fix_order=instruct_fix_next)
                  if len(np.unique(sub_block_counts)) > 1:
                      # Corrected Msg
                      raise ValueError(
                            f"Level {level} (positive index {block_val}) requires sub-blocks "
                            f"defined by level {level + 1} to be uniform size for whole-block shuffling. "
                            f"Indices subset starting with: {current_original_indices[:5]}.... "
                            f"Sub-block IDs: {unique_sub_blocks}. "
                            f"Sub-block sizes: {sub_block_counts}."
                         )
                  sub_block_indices_list = [ current_original_indices[sub_block_inverse == i] for i in range(n_sub_blocks) ]
                  shuffled_sub_block_order = rng.permutation(n_sub_blocks)
                  # Pass down fix instruction based on the SUB-BLOCK'S sign
                  permuted_sub_blocks = [ permute_indices_recursive(sub_block_indices_list[i], level + 1, eb_matrix, rng, parent_instructed_fix_order=(unique_sub_blocks[i]<0)) for i in range(n_sub_blocks) ]
                  return np.concatenate([permuted_sub_blocks[idx] for idx in shuffled_sub_block_order])
             else: # block_val < 0
                  # Negative: Recurse, instructing next level to fix order.
                  return permute_indices_recursive(current_original_indices, level + 1, eb_matrix, rng, parent_instructed_fix_order=True)

    # --- Multi Block Logic ---
    else: # n > 1
        signs = np.sign(unique_blocks)
        if np.any(unique_blocks == 0):
             raise ValueError(f"Block index 0 found at level {level} among {unique_blocks} for indices {current_original_indices[:5]}..., which is not supported.")
        if len(np.unique(signs)) > 1:
             raise ValueError(
                f"Level {level}: Mixed positive/negative block indices found "
                f"({unique_blocks}) within the same parent block structure "
                f"for indices starting with {current_original_indices[:5]}..., which is ambiguous and not supported by PALM."
            )

        # *** Prioritize Parent Instruction & Last Level Check ***
        if parent_instructed_fix_order:
             # Parent said fix order -> MUST concatenate this level in order. Recurse within.
             permuted_indices_list = []
             for i, block_val_i in enumerate(unique_blocks):
                  mask = (block_inverse_indices == i)
                  indices_in_this_block_i = current_original_indices[mask]
                  # Recurse, passing instruction based on this block's sign
                  instruct_fix_i = (block_val_i < 0)
                  permuted_subset = permute_indices_recursive(indices_in_this_block_i, level + 1, eb_matrix, rng, parent_instructed_fix_order=instruct_fix_i)
                  permuted_indices_list.append(permuted_subset)
             return np.concatenate(permuted_indices_list) # Concat in order

        elif is_last_defined_level:
             # Parent allowed shuffle AND this is the last level. Terminate based on signs.
             if signs[0] > 0:
                  # Freely permute all involved indices together.
                  return rng.permutation(current_original_indices)
             else: # signs[0] < 0
                  # Identity for each block, concatenate in order.
                  permuted_indices_list = [np.copy(current_original_indices[block_inverse_indices == i])
                                           for i in range(n_unique_blocks_at_level)]
                  return np.concatenate(permuted_indices_list)
        else:
             # Intermediate level AND parent allowed shuffle.
             # Recurse within each block. Concatenate results based on *this* level's signs (Original V1 logic).
             permuted_indices_list = []
             for i, block_val_i in enumerate(unique_blocks):
                  mask = (block_inverse_indices == i)
                  indices_in_this_block_i = current_original_indices[mask]
                  instruct_fix_i = (block_val_i < 0)
                  permuted_subset = permute_indices_recursive(indices_in_this_block_i, level + 1, eb_matrix, rng, parent_instructed_fix_order=instruct_fix_i)
                  permuted_indices_list.append(permuted_subset)

             # Use original concatenation logic because parent_fix is False
             if signs[0] > 0: # Shuffle order
                  shuffled_block_order = rng.permutation(n_unique_blocks_at_level)
                  return np.concatenate([permuted_indices_list[idx] for idx in shuffled_block_order])
             else: # Preserve order
                  return np.concatenate(permuted_indices_list)


def yield_permuted_design(design, n_permutations, random_state, exchangeability_matrix=None, within=None, whole=None):
    """Generator for permuting the design matrix per PALM documentation.

    Handles free exchange, within-block, whole-block, combined within/whole,
    and multi-level exchangeability via positive/negative indices.
    Docs: https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/PALM(2f)ExchangeabilityBlocks.html

    Args:
        design (np.ndarray): Design matrix. Shape (n_samples, n_features).
        n_permutations (int): Number of permutations to generate.
        random_state (int or np.random.Generator or None): Seed or Generator
            for the random number generator.
        exchangeability_matrix (np.ndarray or None): Matrix or vector defining
            exchangeability blocks. Shape (n_samples,) or (n_samples, n_levels).
            If None, free exchange is assumed. Defaults to None.
        within (bool | None): For single-column blocks, allow shuffling within blocks.
                       If None (default): Behavior depends on 'whole'. If 'whole' is also None or False,
                       defaults to True. If 'whole' is True, defaults to False.
                       Ignored if exchangeability_matrix has >1 column or if None.
        whole (bool | None): For single-column blocks, shuffle blocks as wholes.
                      If None (default): Defaults to False.
                      Ignored if exchangeability_matrix has >1 column or if None.

    Yields:
        np.ndarray: A permuted version of the design matrix.

    Raises:
        ValueError: If inputs are inconsistent (e.g., non-uniform block sizes
                    required for whole-block shuffling, ambiguous multi-col structure,
                    zero indices in eb_matrix).
        TypeError: If design or exchangeability_matrix is not a numpy array or
                   if eb_matrix contains non-numeric data.
    """
    # --- Input Validation ---
    if not isinstance(design, np.ndarray):
        raise TypeError("design must be a numpy array.")
    if design.ndim != 2:
        raise ValueError(f"design must be 2D (samples x features), got shape {design.shape}")

    n_samples = design.shape[0]
    if n_samples == 0:
         # Handle empty design matrix - yield nothing or raise error?
         # Let's yield nothing as n_permutations would be irrelevant.
         return

    # Initialize RNG
    if isinstance(random_state, np.random.Generator):
        rng = random_state
    else:
        rng = np.random.default_rng(random_state)

    original_indices = np.arange(n_samples)

    # --- Preprocess exchangeability_matrix (eb_matrix) ---
    is_eb_provided = exchangeability_matrix is not None
    eb_matrix = None
    n_levels = 0
    use_flags = False # Default

    if is_eb_provided:
        if not isinstance(exchangeability_matrix, np.ndarray):
            raise TypeError("exchangeability_matrix must be a numpy array.")
        if exchangeability_matrix.size == 0 and n_samples > 0 :
             raise ValueError("exchangeability_matrix is empty but design matrix is not.")
        if exchangeability_matrix.size > 0:
            if not np.issubdtype(exchangeability_matrix.dtype, np.number):
                 raise TypeError("exchangeability_matrix must contain numeric indices.")

            # Check for non-integer values that aren't trivially convertible (e.g., 1.5 vs 1.0)
            if not np.all(np.mod(exchangeability_matrix, 1) == 0):
                 # Check more robustly if conversion is possible without loss
                 try:
                     int_eb = exchangeability_matrix.astype(int)
                     if not np.all(np.isclose(exchangeability_matrix, int_eb)):
                         raise ValueError("Non-integer values found in exchangeability_matrix.")
                     eb_matrix = int_eb
                 except (ValueError, TypeError):
                      raise ValueError("Non-integer values found in exchangeability_matrix.")
            else:
                 eb_matrix = exchangeability_matrix.astype(int)

            if eb_matrix.shape[0] != n_samples:
                raise ValueError(f"exchangeability_matrix rows ({eb_matrix.shape[0]}) "
                                 f"must match design matrix rows ({n_samples}).")
            if eb_matrix.ndim == 1:
                eb_matrix = eb_matrix.reshape(-1, 1)
            elif eb_matrix.ndim > 2:
                raise ValueError("exchangeability_matrix cannot have more than 2 dimensions.")
            elif eb_matrix.ndim == 0: # Should be caught by shape[0] check if n_samples > 0
                 raise ValueError("exchangeability_matrix cannot be 0-dimensional.")

            n_levels = eb_matrix.shape[1]
            use_flags = (n_levels == 1) # Flags only relevant if effectively single-level

            # Final check for 0 indices which are unsupported by PALM logic
            if np.any(eb_matrix == 0):
                 raise ValueError("Exchangeability matrix contains index 0, which is not supported (use positive/negative integers).")

    # --- Determine Effective within/whole for single-level ---
    eff_within = within
    eff_whole = whole
    if use_flags:
        # Apply default logic only if flags are relevant (single level)
        if eff_whole is None:
            eff_whole = False
        if eff_within is None:
            eff_within = not eff_whole # Default within=True unless whole=True

    # --- Define the permutation function for one iteration ---
    def get_permuted_indices():
        if not is_eb_provided or eb_matrix is None:
            # Case 0: Free exchange (no eb_matrix provided or it was empty)
            return rng.permutation(original_indices)

        # --- Determine strategy based on levels and flags ---
        if use_flags:
            # --- Case 1: Single Level - Use Flags ---
            block_ids = eb_matrix[:, 0]
            unique_blocks, inverse = np.unique(block_ids, return_inverse=True)
            n_unique_blocks = len(unique_blocks)

            # Trivial case: only one block behaves like free exchange
            if n_unique_blocks <= 1:
                 return rng.permutation(original_indices)

            if eff_within and eff_whole:
                # Simultaneous within & whole -> Treat as free exchange
                # (Based on VG interpretation suggesting equivalence to simplest case)
                return rng.permutation(original_indices)
            elif eff_whole:
                # Whole-block shuffling
                unique_blocks, inverse, counts = np.unique(block_ids, return_inverse=True, return_counts=True)
                if len(np.unique(counts)) > 1:
                     raise ValueError(
                        "Whole-block shuffling requires all blocks to be the same size. "
                        f"Found sizes: {counts} for blocks {unique_blocks}"
                     )
                n_blocks = len(unique_blocks)
                # Group original indices by block ID
                blocks_indices = [original_indices[inverse == i] for i in range(n_blocks)]
                # Shuffle the order of the blocks
                shuffled_block_order = rng.permutation(n_blocks)
                # Concatenate blocks in the new shuffled order
                return np.concatenate([blocks_indices[i] for i in shuffled_block_order])
            elif eff_within:
                # Within-block shuffling
                permuted_indices = np.copy(original_indices) # Start with identity
                for i in range(n_unique_blocks):
                    mask = (inverse == i)
                    indices_this_block = original_indices[mask]
                    # Permute the indices *within* this block
                    shuffled_subset = rng.permutation(indices_this_block)
                    # Assign the permuted indices back to the original positions of the block
                    permuted_indices[mask] = shuffled_subset
                return permuted_indices
            else: # within=False, whole=False explicitly set
                # This state isn't clearly defined by PALM for permutations.
                # Defaulting to free exchange as the least restrictive assumption.
                return rng.permutation(original_indices)
        else:
            # --- Case 2: Multi-Level (Ignore flags) ---
            # Call the recursive helper starting at level 0
            return permute_indices_recursive(original_indices, 0, eb_matrix, rng)

    # --- Generator Loop ---
    for i in range(n_permutations):
        permuted_row_indices = get_permuted_indices()
        # Check if the permutation is valid before yielding
        if len(permuted_row_indices) != n_samples:
             raise RuntimeError(f"Permutation {i+1} generated incorrect number of indices: "
                                f"{len(permuted_row_indices)}, expected {n_samples}")
        yield design[permuted_row_indices, :]


def compute_p_values_accel_tail(observed, null_dist, two_tailed=True):
    """
    Compute p-values using empirical counts and, when appropriate,
    refine the tail (p <= 0.075) via a generalized Pareto distribution (GPD)
    fit on the upper tail of the null distribution.

    Parameters
    ----------
    observed : np.ndarray, shape (n_elements,)
        Observed statistic for each element.
    null_dist : np.ndarray, shape (n_permutations,)
        Null distribution (the same for all voxels).
    two_tailed : bool, default True
        Whether to use a two-tailed test. In that case, the absolute values
        of observed and null_dist are used.

    Returns
    -------
    p_final : np.ndarray, shape (n_elements,)
        The computed (and possibly refined) p-values.
    """
    # Use absolute values if two_tailed
    if two_tailed:
        observed = np.abs(observed)
        null_dist = np.abs(null_dist)
    
    n_perms = null_dist.size

    # Compute empirical p-values: count the number of nulls >= each observed.
    exceedances = np.sum(null_dist[None, :] >= observed[:, None], axis=1)
    p_emp = (exceedances + 1) / (n_perms + 1)
    
    # If no p_emp is <= 0.075, nothing is extreme enough; return empirical p-values.
    if not np.any(p_emp <= 0.075):
        return p_emp

    # --- Fit a GPD to the tail of null_dist above a threshold ---
    # Start with the 75th percentile as our threshold.
    threshold_percentile = 75
    max_iter = 10
    good_fit_found = False
    threshold = np.percentile(null_dist, threshold_percentile)
    
    for _ in range(max_iter):
        # Select tail of null distribution: values >= threshold
        tail = null_dist[null_dist >= threshold]
        
        # If too few points are in the tail, break and stick with empirical p-values.
        if tail.size < 10:
            break

        # Fit the GPD to the excesses (tail minus threshold)
        excesses = tail - threshold
        fit_params = genpareto.fit(excesses)  # returns (shape, loc, scale)
        
        # Use a KS test to check goodness-of-fit.
        ks_stat, ks_pvalue = kstest(excesses, 'genpareto', args=fit_params)
        if ks_pvalue > 0.05:
            good_fit_found = True
            break
        else:
            # Increase threshold percentile for a potentially better fit.
            threshold_percentile += (((100*(1-0.075)) - threshold_percentile) / max_iter) - 0.01 # Don't want the threshold to exceed 92.5, which is the cutoff mask for the voxels we will be using this GPD fit on.
            threshold = np.percentile(null_dist, threshold_percentile)
    
    # If no good GPD fit was found, return the empirical p-values.
    if not good_fit_found:
        return p_emp

    # Compute the tail probability (fraction of nulls above threshold)
    tail_prob = np.mean(null_dist >= threshold)
    
    # For those observed values that are extreme (p_emp <= 0.075) and exceed the threshold,
    # we recompute the p-value using the fitted GPD.
    p_final = p_emp.copy()
    mask = (p_emp <= 0.075) & (observed >= threshold)
    
    if np.any(mask):
        excess_obs = observed[mask] - threshold
        gpd_cdf_vals = genpareto.cdf(excess_obs, *fit_params)
        p_gpd = tail_prob * (1 - gpd_cdf_vals)
        p_final[mask] = p_gpd

    return p_final


class TfceStatsManager:
    def __init__(self, true_stats, mask_img, two_tailed=True):
        # Compute the true TFCE-transformed statistics
        self.mask_img = mask_img  # keep for later use in update
        self.masker = NiftiMasker(mask_img).fit()
        self.two_tailed = two_tailed
        # Initialize state variables (will be set on first update call)
        self.exceedances_tfce = None
        self.max_stat_dist_tfce = None
        self.true_stats_tfce = apply_tfce(self.masker.inverse_transform(true_stats)).get_fdata()
        self.true_stats_tfce = self.true_stats_tfce[self.mask_img.get_fdata() != 0]

    def update(self, permuted_stats, permutation_idx):
        # Transform the permuted stats with TFCE
        permuted_stats_tfce = apply_tfce(self.masker.inverse_transform(permuted_stats)).get_fdata()
        permuted_stats_tfce = permuted_stats_tfce[self.mask_img.get_fdata() != 0]
        # On the first iteration, initialize state arrays/scalars
        if permutation_idx == 0:
            self.exceedances_tfce = np.zeros(permuted_stats_tfce.shape)
            if self.two_tailed:
                self.max_stat_dist_tfce = np.max(np.abs(permuted_stats_tfce))
            else:
                self.max_stat_dist_tfce = np.max(permuted_stats_tfce)
        else:
            if self.two_tailed:
                # Update exceedances: count where abs(permuted) >= abs(true)
                self.exceedances_tfce += (np.abs(permuted_stats_tfce) >= np.abs(self.true_stats_tfce))
                # Concatenate the new max value using pd.concat equivalent (if arrays) or np.hstack
                self.max_stat_dist_tfce = np.hstack([
                    np.max(np.abs(permuted_stats_tfce)),
                    self.max_stat_dist_tfce
                ])
            else:
                self.exceedances_tfce += (permuted_stats_tfce >= self.true_stats_tfce)
                self.max_stat_dist_tfce = np.hstack([
                    np.max(permuted_stats_tfce),
                    self.max_stat_dist_tfce
                ])

    def finalize(self, n_permutations, accel_tail=True):
        # Compute uncorrected p-values for TFCE
        unc_p_tfce = (self.exceedances_tfce + 1) / (n_permutations + 1)
        # Apply FDR correction (using BH procedure)
        fdr_p_tfce = fdrcorrection(unc_p_tfce)[1]
        # Compute FWE p-values (using accelerated tail estimation if desired)
        if accel_tail:
            fwe_p_tfce = compute_p_values_accel_tail(self.true_stats_tfce,
                                                     self.max_stat_dist_tfce,
                                                     two_tailed=self.two_tailed)
        else:
            if self.two_tailed:
                fwe_p_tfce = (np.sum(self.max_stat_dist_tfce[None, :] >= np.abs(self.true_stats_tfce[:, None]), axis=1) + 1) / (n_permutations + 1)
            else:
                fwe_p_tfce = (np.sum(self.max_stat_dist_tfce[None, :] >= self.true_stats_tfce[:, None], axis=1) + 1) / (n_permutations + 1)
        print(unc_p_tfce.shape, fdr_p_tfce.shape, fwe_p_tfce.shape)
        print(np.count_nonzero(unc_p_tfce), np.count_nonzero(fdr_p_tfce), np.count_nonzero(fwe_p_tfce))
        # Somethings going wrong for fwe_p_tfce-- the shape is ()!
        # What's going wrong? And can you fix it with a very simple change?
        return unc_p_tfce, fdr_p_tfce, fwe_p_tfce


def get_vg_vector(exchangeability_matrix, within=True, whole=False):
    """
    Calculates the variance group (VG) vector based on exchangeability rules.

    Args:
        exchangeability_matrix (np.ndarray): 
            A 1D or 2D numpy array defining exchangeability blocks.
            - For 1D: Integer indices defining blocks. 'within' and 'whole' flags matter.
            - For 2D: Defines nested exchangeability. Flags are ignored.
              - Positive index in col k: Sub-indices in col k+1 shuffle as a whole.
              - Negative index in col k: Sub-indices in col k+1 shuffle within block.
        within (bool, optional): 
            If True and exchangeability_matrix is 1D and whole=False, 
            indicates within-block exchangeability. Defaults to True.
        whole (bool, optional): 
            If True and exchangeability_matrix is 1D, indicates whole-block
            exchangeability. Overrides 'within' if both are True for VG calc.
            Defaults to False.

    Returns:
        np.ndarray: A 1D numpy array of unique integer identifiers (starting from 1)
                    defining the variance groups (vg_vector) for each observation.

    Raises:
        ValueError: If inputs are inconsistent (e.g., non-uniform block sizes
                    required for whole-block shuffling, ambiguous multi-col structure).
        TypeError: If exchangeability_matrix is not a numpy array.
    """

    if not isinstance(exchangeability_matrix, np.ndarray):
        raise TypeError("exchangeability_matrix must be a numpy array.")

    # Check if dtype is already integer
    if not np.issubdtype(exchangeability_matrix.dtype, np.integer):
        # If not integer, check if it contains only integer-like values (e.g., floats like 1.0)
        try:
            # Use np.mod and check closeness to 0 for float precision issues
            is_integer_like = np.all(np.isclose(np.mod(exchangeability_matrix, 1), 0))
        except TypeError:
            # This catches errors if np.mod fails (e.g., non-numeric types)
            raise ValueError("exchangeability_matrix must contain numeric integer-like indices.")

        if is_integer_like:
            # If all are integer-like, convert safely
            exchangeability_matrix = exchangeability_matrix.astype(int)
        else:
            # If any are truly non-integer floats (like 1.5), raise specific error
            raise ValueError("Non-integer values found in exchangeability_matrix.")

    # Store original dimension and force to 2D for consistent processing
    original_ndim = exchangeability_matrix.ndim
    if original_ndim == 0:
        raise ValueError("exchangeability_matrix cannot be 0-dimensional.")
    elif original_ndim == 1:
        # Reshape 1D array to a 2D array with one column
        eb_matrix = exchangeability_matrix.reshape(-1, 1)
    else:
        eb_matrix = exchangeability_matrix

    n_observations = eb_matrix.shape[0]
    n_levels = eb_matrix.shape[1]

    if n_observations == 0:
        return np.array([], dtype=int)
    if n_observations == 1:
        return np.ones(1, dtype=int)

    # --- Determine the effective VG rule ---
    
    use_flags = (original_ndim == 1)
    # According to the description, multi-column structure overrides flags
    if n_levels > 1:
         use_flags = False
         
    vg_vector = np.ones(n_observations, dtype=int) # Default to single group

    # --- Case 1: Use Flags (Original matrix was 1D) ---
    if use_flags:
        block_ids = eb_matrix[:, 0]
        # Handle potentially non-contiguous block IDs by mapping them
        unique_blocks, block_indices = np.unique(block_ids, return_inverse=True)
        
        # Calculate counts based on the mapped indices
        block_counts = np.bincount(block_indices)

        # If only one effective block, it's always a single VG
        if len(unique_blocks) <= 1:
             return np.ones(n_observations, dtype=int) # Correctly handles single block case

        if whole and within:
            # Simultaneous whole- and within-block => freely exchangeable => single VG
            return np.ones(n_observations, dtype=int)
        elif whole:
            # Whole-block shuffling (-whole flag)
            # Check for uniform block sizes using the calculated counts
            if len(np.unique(block_counts)) > 1:
                raise ValueError(
                    "Whole-block shuffling requires all blocks to be the same size. "
                    f"Found sizes: {block_counts}" # Show counts for unique blocks
                )
            block_size = block_counts[0]
            # VG = position within block (1 to block_size)
            # Generate VG based on position within original blocks
            temp_vg = np.zeros(n_observations, dtype=int)
            current_pos_in_block = {} # Key: block_id, Value: next position
            for i in range(n_observations):
                block_val = block_ids[i]
                pos = current_pos_in_block.get(block_val, 0)
                temp_vg[i] = pos + 1
                current_pos_in_block[block_val] = pos + 1
            vg_vector = temp_vg

        elif within:
            # Within-block shuffling (-within flag, default)
            # VG = block index (1-based) based on unique values encountered
            vg_vector = block_indices + 1
        else:
            # Neither within nor whole specified -> freely exchangeable -> single VG
             return np.ones(n_observations, dtype=int)


    # --- Case 2: Multi-Column Matrix (Flags ignored) ---
    elif n_levels > 1:
        col_0 = eb_matrix[:, 0]
        col_1 = eb_matrix[:, 1]
        
        # Determine unique groups based on first column
        unique_l0, indices_l0 = np.unique(col_0, return_inverse=True)

        # Check if the first level implies whole or within block shuffling
        # Assuming uniformity *within each block* defined by unique_l0
        
        # Check for mixed signs *across* blocks if multiple l0 blocks exist
        if len(unique_l0) > 1 and (np.any(unique_l0 > 0) and np.any(unique_l0 < 0)):
             raise ValueError(
                "Multi-column exchangeability matrix contains mixed positive/negative "
                "indices in the first column across different top-level blocks. "
                "Automatic VG determination for this specific structure is not supported."
             )
        
        # Determine effective rule (all positive or all negative in first relevant column)
        first_sign = col_0[0] # Check based on the first entry's sign
        all_positive = np.all(col_0 > 0)
        all_negative = np.all(col_0 < 0)

        if not (all_positive or all_negative):
             # If not uniformly positive or negative, check if it's just one block type
             if len(unique_l0) == 1:
                 all_positive = unique_l0[0] > 0
                 all_negative = unique_l0[0] < 0
             else: # Mixed signs within a block or across blocks was checked earlier
                 raise ValueError(
                     "Ambiguous multi-column structure: first column indices are not "
                     "consistently positive or negative."
                 )


        if all_positive:
             # Positive indices in col 0 -> Whole-block shuffling implied for col 1 groups
             # VG = position within the blocks defined by col 1
             
             # Need to determine block sizes based on col_1 *within* each col_0 group
             block_sizes = []
             temp_vg = np.zeros(n_observations, dtype=int)
             
             # Map positions within each unique block defined by col_1
             current_pos_in_sub_block = {} # key=col_1 value, val=next_pos
             for i in range(n_observations):
                 sub_block_val = col_1[i]
                 pos = current_pos_in_sub_block.get(sub_block_val, 0)
                 temp_vg[i] = pos + 1
                 current_pos_in_sub_block[sub_block_val] = pos + 1

             # Now check uniformity of sizes for blocks defined by col_1
             unique_sub_blocks, sub_block_indices, sub_block_counts = np.unique(
                 col_1, return_inverse=True, return_counts=True
             )
             
             # Special case: If overall structure results in only one group -> VG=1
             # e.g. [[1,1],[1,2],[1,3]] -> sub_block_counts = [1,1,1] -> block_size=1
             if len(np.unique(sub_block_counts)) > 1:
                  # Check if it's just trivial blocks of size 1
                  if not np.all(sub_block_counts == 1):
                      raise ValueError(
                         "Whole-block shuffling implied by positive indices requires "
                         "effective sub-blocks (from the second level) to be the same size. "
                         f"Found sizes based on column 1: {sub_block_counts}"
                      )
             
             block_size = sub_block_counts[0]
             # Special case check: If block size is 1, it's like free exchangeability
             if block_size == 1:
                 return np.ones(n_observations, dtype=int)
             
             vg_vector = temp_vg # Use the calculated positions

        elif all_negative:
            # Negative indices in col 0 -> Within-block shuffling implied for col 1 groups
            # VG = index of the block defined by col 1 (make unique IDs 1-based)
            unique_sub_blocks, sub_block_indices = np.unique(
                col_1, return_inverse=True
            )
            
            # If only one effective sub-block, implies single VG
            if len(unique_sub_blocks) <= 1:
                return np.ones(n_observations, dtype=int)
                
            vg_vector = sub_block_indices + 1
            
        # The case where neither all_positive nor all_negative should be caught by prior checks

    # --- Fallback for single level (if somehow missed) ---
    elif n_levels == 1:
         # Treat as 1D case with default flags (within=True, whole=False)
         # This case should theoretically be handled by use_flags=True path
         block_ids = eb_matrix[:, 0]
         unique_blocks, block_indices = np.unique(block_ids, return_inverse=True)
         if len(unique_blocks) <= 1:
             return np.ones(n_observations, dtype=int)
         else:
             vg_vector = block_indices + 1 # Default 'within' logic


    return vg_vector.astype(int)


###############################

# --- Internal Analysis Class ---
class _SpatialCorrelationAnalysis:
    """Internal class to manage the spatial correlation analysis steps."""

    def __init__(self, datasets_input: Union[Dataset, List[Dataset]],
                 reference_maps_input: Optional[Union[str, nib.Nifti1Image, np.ndarray, List[Union[str, nib.Nifti1Image, np.ndarray]]]],
                 two_tailed: bool):

        self.datasets_input = datasets_input
        self.reference_maps_input = reference_maps_input
        self.two_tailed = two_tailed

        # Initialize state variables
        self.datasets: List[Dataset] = []
        self.final_reference_maps: List[np.ndarray] = []
        self.n_datasets: int = 0
        self.n_references: int = 0
        self.n_permutations: int = 0
        self.target_feature_shape: Optional[int] = None
        self.common_masker: Optional[NiftiMasker] = None

        self.true_stats_list: List[np.ndarray] = []
        self.true_corr_ds_ds: Optional[np.ndarray] = None
        self.true_corr_ds_ref: Optional[np.ndarray] = None
        self.permuted_corrs_ds_ds: Optional[np.ndarray] = None
        self.permuted_corrs_ds_ref: Optional[np.ndarray] = None

    def _setup_and_validate(self) -> bool:
        """Loads data, standardizes inputs, handles NIfTI/masking, validates shapes."""
        print("Setting up and validating analysis inputs...")
        # 1. Standardize Datasets
        if not isinstance(self.datasets_input, list):
            self.datasets = [self.datasets_input]
        else:
            self.datasets = self.datasets_input
        if not self.datasets:
            raise ValueError("Dataset list cannot be empty.")
        for i, ds in enumerate(self.datasets):
             if not isinstance(ds, Dataset):
                 raise TypeError(f"Item {i} in datasets list is not a Dataset object.")
        self.n_datasets = len(self.datasets)

        # 2. Standardize Reference Maps
        if self.reference_maps_input is None:
            ref_maps_list_raw = []
        elif not isinstance(self.reference_maps_input, list):
            ref_maps_list_raw = [self.reference_maps_input]
        else:
            ref_maps_list_raw = self.reference_maps_input
        self.n_references = len(ref_maps_list_raw)
        print(f"Found {self.n_datasets} dataset(s) and {self.n_references} reference map(s).")

        # 3. Handle Trivial Cases (e.g., Case D)
        if self.n_datasets == 0 or (self.n_datasets < 2 and self.n_references == 0):
            warnings.warn("Insufficient inputs for correlation analysis (need >= 2 datasets or >= 1 dataset and >= 1 reference map).")
            return False # Cannot proceed

        # 4. Load Dataset Data (calls dataset.load_data())
        for ds in self.datasets:
            ds.load_data()

        # 5. Handle NIfTI Consistency and Masking
        self._prepare_common_masker()
        self._process_reference_maps(ref_maps_list_raw) # Applies mask, populates self.final_reference_maps

        # 6. Shape Validation
        if not self._validate_shapes():
             return False # Stop if shapes mismatch

        # 7. Determine Number of Permutations
        self.n_permutations = min(ds.n_permutations for ds in self.datasets)
        print(f"Using minimum n_permutations across datasets: {self.n_permutations}")
        if self.n_permutations <= 0:
            warnings.warn("n_permutations is zero or less. Cannot run permutation testing.")
            return False # Cannot proceed

        print("Setup and validation complete.")
        return True # OK to proceed

    def _prepare_common_masker(self):
        """Determines and stores a common NiftiMasker if NIfTI inputs exist."""
        any_dataset_is_nifti = any(ds.is_nifti for ds in self.datasets)
        any_reference_is_nifti = any(is_nifti_like(ref) for ref in (self.reference_maps_input if isinstance(self.reference_maps_input, list) else [self.reference_maps_input]) if self.reference_maps_input is not None)

        if any_dataset_is_nifti:
            first_nifti_ds_masker = next((ds.masker for ds in self.datasets if ds.is_nifti and ds.masker), None)
            if first_nifti_ds_masker:
                self.common_masker = first_nifti_ds_masker
                print(f"Using mask from first available NIfTI dataset for consistent space.")
            else:
                warnings.warn("NIfTI datasets found, but no mask determined. Check Dataset.load_data().")
        elif any_reference_is_nifti:
             first_ref_nifti = next(ref for ref in (self.reference_maps_input if isinstance(self.reference_maps_input, list) else [self.reference_maps_input]) if is_nifti_like(ref))
             first_ref_nifti_img = load_nifti_if_not_already_nifti(first_ref_nifti)
             warnings.warn("Ref maps NIfTI, datasets not. Creating mask from first NIfTI ref.")
             self.common_masker = NiftiMasker()
             self.common_masker.fit(first_ref_nifti_img)

    def _process_reference_maps(self, ref_maps_list_raw: List):
        """Applies common mask (if any) to reference maps and stores them as 1D arrays."""
        self.final_reference_maps = []
        if self.n_references == 0: return
        print("Processing reference maps...")
        for i, ref_map_input in enumerate(ref_maps_list_raw):
            if is_nifti_like(ref_map_input):
                ref_img = load_nifti_if_not_already_nifti(ref_map_input)
                if self.common_masker:
                    try:
                        masked_ref = self.common_masker.transform(ref_img)
                        self.final_reference_maps.append(masked_ref.ravel())
                        print(f"  - Ref map {i+1} (NIfTI) masked. Shape: {masked_ref.ravel().shape}")
                    except Exception as e:
                        raise ValueError(f"Failed to transform ref NIfTI {i+1} with common mask.") from e
                else:
                     warnings.warn(f"Ref map {i+1} is NIfTI, but no common masker. Using raw data.")
                     try:
                        self.final_reference_maps.append(ref_img.get_fdata().ravel())
                     except Exception as e:
                        raise ValueError("Failed to get raw data from reference NIfTI.") from e
            elif isinstance(ref_map_input, np.ndarray):
                if self.common_masker:
                    warnings.warn(f"Ref map {i+1} is NumPy array, ensure it matches masked space.")
                ref_map = ref_map_input.ravel()
                self.final_reference_maps.append(ref_map)
                print(f"  - Ref map {i+1} (NumPy) used. Shape: {ref_map.shape}")
            else:
                 raise TypeError(f"Unsupported type for ref map {i+1}: {type(ref_map_input)}")

    def _validate_shapes(self) -> bool:
        """Checks if all dataset data and reference maps have consistent feature dimensions."""
        print("Validating shapes...")
        if self.n_datasets > 0:
            first_ds_data = self.datasets[0].data
            if first_ds_data is None or first_ds_data.ndim != 2:
                 raise ValueError("First dataset data is None or not 2D after loading.")
            self.target_feature_shape = first_ds_data.shape[1] # features dimension
            print(f"  - Target feature shape (from dataset 1): {self.target_feature_shape}")
            for i, ds in enumerate(self.datasets[1:], 1):
                 if ds.data is None or ds.data.ndim != 2:
                     raise ValueError(f"Dataset {i+1} data is None or not 2D after loading.")
                 if ds.data.shape[1] != self.target_feature_shape:
                     raise ValueError(f"Shape mismatch: Dataset {i+1} has {ds.data.shape[1]} features, expected {self.target_feature_shape}.")
        elif self.n_references > 0: # No datasets, derive from references
            self.target_feature_shape = self.final_reference_maps[0].shape[0]
            print(f"  - Target feature shape (from reference 1): {self.target_feature_shape}")
        else: # Should have been caught by trivial case check
            print("  - No data found to validate shapes.")
            return True # Or False? If no data, maybe false. Let's stick with True as trivial case handles exit.

        # Validate references against target shape
        for i, ref_map in enumerate(self.final_reference_maps):
             if ref_map.shape[0] != self.target_feature_shape:
                 raise ValueError(f"Shape mismatch: Reference map {i+1} has {ref_map.shape[0]} features, expected {self.target_feature_shape}.")

        print("  - Shape validation successful.")
        return True

    def calculate_true_statistics(self):
        """Calculates the true statistic map for each dataset."""
        print("Calculating true statistics...")
        self.true_stats_list = []
        for i, dataset in enumerate(self.datasets):
            print(f"  Processing dataset {i+1}/{self.n_datasets}...")
            if dataset.data is None or dataset.design is None or dataset.contrast is None:
                 raise ValueError(f"Dataset {i+1} not loaded properly.")

            vg_vec = dataset.vg_vector
            n_groups = None
            # Note: Assumes get_vg_vector exists and handles None input correctly
            if vg_vec is None and dataset.exchangeability_matrix is not None and dataset.vg_auto:
                 vg_vec = get_vg_vector(dataset.exchangeability_matrix, within=dataset.within, whole=dataset.whole)

            stat_args = [dataset.data, dataset.design, dataset.contrast]
            stat_kwargs = {} # For potential future TFCE args etc.
            if dataset.tfce:
                 # This assumes stat_function knows how to handle tfce=True
                 # Or you might need neighbor info passed here. Adjust as needed.
                 stat_kwargs['tfce'] = True
                 print("    - TFCE flag is True (assuming stat_function handles it).")


            if vg_vec is not None:
                n_groups = len(np.unique(vg_vec))
                # Assuming stat_function signature adapts
                stat_args.extend([vg_vec, n_groups])
                print(f"    - Calculating with variance groups. Groups: {n_groups}")
                dataset.true_stats = dataset.stat_function(*stat_args, **stat_kwargs)
            else:
                print(f"    - Calculating without variance groups.")
                dataset.true_stats = dataset.stat_function(*stat_args, **stat_kwargs)

            if dataset.true_stats.ndim > 1:
                 dataset.true_stats = dataset.true_stats.ravel()
            self.true_stats_list.append(dataset.true_stats)
            print(f"    - Stat map shape: {dataset.true_stats.shape}")

    def calculate_true_correlations(self):
        """Calculates the true correlation matrices."""
        print("Calculating true correlations...")
        self.true_corr_ds_ds = None
        if self.n_datasets > 1:
            print("  Calculating dataset-vs-dataset correlations...")
            stacked_ds_stats = np.stack(self.true_stats_list, axis=-1)
            self.true_corr_ds_ds = np.corrcoef(stacked_ds_stats, rowvar=False)
            print(f"    - Shape: {self.true_corr_ds_ds.shape}")

        self.true_corr_ds_ref = None
        if self.n_datasets > 0 and self.n_references > 0:
            print("  Calculating dataset-vs-reference correlations...")
            stacked_ds_stats = np.stack(self.true_stats_list, axis=-1)
            stacked_ref_maps = np.stack(self.final_reference_maps, axis=-1)
            combined_maps = np.hstack((stacked_ds_stats, stacked_ref_maps))
            full_corr_matrix = np.corrcoef(combined_maps, rowvar=False)
            self.true_corr_ds_ref = full_corr_matrix[:self.n_datasets, self.n_datasets:]
            print(f"    - Shape: {self.true_corr_ds_ref.shape}")

    def run_permutations(self):
        """Runs the permutation process and collects permuted correlations."""
        print(f"Running {self.n_permutations} permutations...")
        # Initialize storage
        if self.n_datasets > 1:
            self.permuted_corrs_ds_ds = np.zeros((self.n_permutations, self.n_datasets, self.n_datasets))
        if self.n_datasets > 0 and self.n_references > 0:
            self.permuted_corrs_ds_ref = np.zeros((self.n_permutations, self.n_datasets, self.n_references))

        # Setup generators
        print("  Setting up permutation generators...")
        for i, dataset in enumerate(self.datasets):
            if dataset.data is None or dataset.design is None or dataset.contrast is None or dataset.random_state is None:
                 raise ValueError(f"Dataset {i+1} not initialized for permutations.")
            # Note: Assumes yield_permuted_stats exists
            dataset.permuted_stat_generator = yield_permuted_stats(
                data=dataset.data, design=dataset.design, contrast=dataset.contrast,
                stat_function=dataset.stat_function, n_permutations=self.n_permutations,
                random_state=dataset.random_state, # Use the dataset's RNG state
                exchangeability_matrix=dataset.exchangeability_matrix,
                vg_auto=dataset.vg_auto, vg_vector=dataset.vg_vector,
                within=dataset.within, whole=dataset.whole, flip_signs=dataset.flip_signs,
            )

        # Permutation loop
        print("  Starting permutation loop...")
        permuted_stats_current = [np.zeros(self.target_feature_shape) for _ in range(self.n_datasets)]
        stacked_ref_maps = np.stack(self.final_reference_maps, axis=-1) if self.n_references > 0 else None

        for perm_idx in range(self.n_permutations):
            if (perm_idx + 1) % max(1, self.n_permutations // 10) == 0:
                 print(f"    Permutation {perm_idx + 1}/{self.n_permutations}")

            for i, dataset in enumerate(self.datasets):
                if dataset.permuted_stat_generator is None: raise RuntimeError("Generator missing.")
                try:
                    perm_stat = next(dataset.permuted_stat_generator)
                    permuted_stats_current[i] = perm_stat.ravel()
                except StopIteration:
                    raise RuntimeError(f"Permutation generator ended early at {perm_idx+1}.")

            stacked_perm_stats = np.stack(permuted_stats_current, axis=-1)

            if self.permuted_corrs_ds_ds is not None:
                perm_corr_ds_ds = np.corrcoef(stacked_perm_stats, rowvar=False)
                self.permuted_corrs_ds_ds[perm_idx] = perm_corr_ds_ds

            if self.permuted_corrs_ds_ref is not None:
                combined_maps = np.hstack((stacked_perm_stats, stacked_ref_maps))
                full_perm_corr = np.corrcoef(combined_maps, rowvar=False)
                self.permuted_corrs_ds_ref[perm_idx] = full_perm_corr[:self.n_datasets, self.n_datasets:]

        print("  Permutation loop finished.")

    def _calculate_p_values_internal(self, true_corrs: np.ndarray, permuted_corrs: np.ndarray) -> np.ndarray:
        """Internal helper to calculate p-values from true and permuted correlations."""
        # permuted_corrs shape: (n_perm, n_rows, n_cols)
        # true_corrs shape: (n_rows, n_cols)
        n_perm = permuted_corrs.shape[0] # Use actual number of permutations run

        if self.two_tailed:
            abs_true = np.abs(true_corrs)
            abs_permuted = np.abs(permuted_corrs)
            exceedances = np.sum(abs_permuted >= abs_true[np.newaxis, :, :], axis=0)
        else: # One-tailed (right)
            exceedances = np.sum(permuted_corrs >= true_corrs[np.newaxis, :, :], axis=0)

        p_values = (exceedances + 1) / (n_perm + 1)
        return p_values

    def run_analysis(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """Orchestrates the analysis steps and returns the p-value matrices."""
        if not self._setup_and_validate():
            return None

        self.calculate_true_statistics()
        self.calculate_true_correlations()
        self.run_permutations() # This populates self.permuted_corrs_*

        # Calculate P-values
        print("Calculating final p-values...")
        p_matrix_ds_ds = None
        if self.true_corr_ds_ds is not None and self.permuted_corrs_ds_ds is not None:
            print("  Calculating dataset-vs-dataset p-values...")
            p_matrix_ds_ds = self._calculate_p_values_internal(self.true_corr_ds_ds, self.permuted_corrs_ds_ds)
            np.fill_diagonal(p_matrix_ds_ds, np.nan) # Set diagonal to NaN

        p_matrix_ds_ref = None
        if self.true_corr_ds_ref is not None and self.permuted_corrs_ds_ref is not None:
            print("  Calculating dataset-vs-reference p-values...")
            p_matrix_ds_ref = self._calculate_p_values_internal(self.true_corr_ds_ref, self.permuted_corrs_ds_ref)

        print("Analysis finished.")
        results = {
            'corr_matrix_ds_ds': self.true_corr_ds_ds,
            'corr_matrix_ds_ref': self.true_corr_ds_ref,
            'p_matrix_ds_ds': p_matrix_ds_ds,
            'p_matrix_ds_ref': p_matrix_ds_ref,
            'corr_matrix_perm_ds_ds': self.permuted_corrs_ds_ds,
            'corr_matrix_perm_ds_ref': self.permuted_corrs_ds_ref
        }
        return results